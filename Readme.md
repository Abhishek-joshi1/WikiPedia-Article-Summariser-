Text summarization in Natural Language Processing (NLP) encompasses the use of 
Deep Learning and Machine Learning models to distil extensive textual data into its most 
salient components. This process, applicable to static texts like research papers or dynamic 
streams such as podcasts, employs Speech-to-Text APIs for audio or video sources. As product 
teams integrate Text Summarization APIs and AI Summarization models into AI-powered 
platforms, they create automated summarization tools for various content types, including 
calls, interviews, and legal documents, often referred to as AI summarizers. The focus of this 
project lies in Wikipedia article summarization, aiming to condense lengthy articles into 
concise versions while retaining crucial information and main points. Summarization enhances 
accessibility, facilitating quicker comprehension of topics without necessitating complete 
article reading. It also aids in navigation and comparison across topics, particularly beneficial 
for users seeking rapid overviews or facing time constraints. Summarized articles serve as 
valuable study aids and reference materials for students and researchers alike. Extractive 
summarization, a prominent technique within text summarization, involves identifying and 
extracting key sentences or phrases directly from the source document, without modification. 
This process entails text pre-processing to remove extraneous elements, followed by sentence 
or phrase selection based on relevance using statistical methods, NLP algorithms, or machine 
learning models. Scoring and ranking mechanisms are then employed to prioritize sentences 
or phrases, culminating in the selection of top-ranked components to form the final summary. 

![alt text](image.png)

![alt text](image-1.png)